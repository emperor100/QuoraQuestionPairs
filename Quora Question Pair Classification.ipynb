{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Question Pairs\n",
    "\n",
    "The Goal of the this competition is to find the similarity of the question. i.e. We need to predict the probability that given two questions are duplicate. The Link of the problem is:\n",
    "[Quora Question Pairs](https://www.kaggle.com/c/quora-question-pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import the required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, BatchNormalization, TimeDistributed, Input\n",
    "from keras.layers import Lambda, Activation, Flatten, Conv1D, concatenate\n",
    "\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from keras import initializers\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare the Data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading only 100 datapoint for the experiment. For full training remove [:100] from both the line\n",
    "train = pd.read_csv(\"./quora-question-pairs/train.csv\")[:100]\n",
    "test = pd.read_csv(\"./quora-question-pairs/test.csv\")[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 6)\n",
      "(100, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna('empty')\n",
    "test = test.fillna('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stop_words=True, stem_words=False):\n",
    "    # Clean the text, with the option to remove stop_words and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"\", text)\n",
    "    text = re.sub(r\"What's\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\0k \", \"0000 \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" America \", text)\n",
    "    text = re.sub(r\" USA \", \" America \", text)\n",
    "    text = re.sub(r\" u s \", \" America \", text)\n",
    "    text = re.sub(r\" uk \", \" England \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text)\n",
    "    text = re.sub(r\"india\", \"India\", text)\n",
    "    text = re.sub(r\"china\", \"China\", text)\n",
    "    text = re.sub(r\"chinese\", \"Chinese\", text) \n",
    "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
    "    text = re.sub(r\"intially\", \"initially\", text)\n",
    "    text = re.sub(r\"quora\", \"Quora\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\"KMs\", \" kilometers \", text)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text) \n",
    "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text) \n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"ios\", \"operating system\", text)\n",
    "    text = re.sub(r\"gps\", \"GPS\", text)\n",
    "    text = re.sub(r\"gst\", \"GST\", text)\n",
    "    text = re.sub(r\"programing\", \"programming\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\"dna\", \"DNA\", text)\n",
    "    text = re.sub(r\"III\", \"3\", text) \n",
    "    text = re.sub(r\"the US\", \"America\", text)\n",
    "    text = re.sub(r\"Astrology\", \"astrology\", text)\n",
    "    text = re.sub(r\"Method\", \"method\", text)\n",
    "    text = re.sub(r\"Find\", \"find\", text) \n",
    "    text = re.sub(r\"banglore\", \"Banglore\", text)\n",
    "    text = re.sub(r\" J K \", \" JK \", text)\n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stop_words:\n",
    "        text = text.split()\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_questions(question_list, questions, question_list_name, dataframe):\n",
    "    '''transform questions and display progress'''\n",
    "    for question in questions:\n",
    "        question_list.append(text_to_wordlist(question))\n",
    "        if len(question_list) % 100000 == 0:\n",
    "            progress = len(question_list)/len(dataframe) * 100\n",
    "            print(\"{} is {}% complete.\".format(question_list_name, round(progress, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_question1 = []\n",
    "process_questions(train_question1, train.question1, 'train_question1', train)\n",
    "\n",
    "\n",
    "\n",
    "train_question2 = []\n",
    "process_questions(train_question2, train.question2, 'train_question2', train)\n",
    "\n",
    "\n",
    "\n",
    "test_question1 = []\n",
    "process_questions(test_question1, test.question1, 'test_question1', test)\n",
    "\n",
    "test_question2 = []\n",
    "process_questions(test_question2, test.question2, 'test_question2', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.0\n",
      "22.0\n",
      "22.024999999999977\n",
      "26.00500000000008\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "for question in train_question1:\n",
    "    lengths.append(len(question.split()))\n",
    "\n",
    "for question in train_question2:\n",
    "    lengths.append(len(question.split()))\n",
    "\n",
    "# Create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])\n",
    "\n",
    "\n",
    "\n",
    "lengths.counts.describe()\n",
    "\n",
    "\n",
    "print(np.percentile(lengths.counts, 99.0))\n",
    "print(np.percentile(lengths.counts, 99.4))\n",
    "print(np.percentile(lengths.counts, 99.5))\n",
    "print(np.percentile(lengths.counts, 99.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting is complete.\n",
      "train_question1 is complete.\n",
      "train_question2 is complete\n"
     ]
    }
   ],
   "source": [
    "# tokenize the words for all of the questions\n",
    "all_questions = train_question1 + train_question2 + test_question1 + test_question2\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_questions)\n",
    "print(\"Fitting is complete.\")\n",
    "train_question1_word_sequences = tokenizer.texts_to_sequences(train_question1)\n",
    "print(\"train_question1 is complete.\")\n",
    "train_question2_word_sequences = tokenizer.texts_to_sequences(train_question2)\n",
    "print(\"train_question2 is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_question1 is complete.\n",
      "test_question2 is complete.\n"
     ]
    }
   ],
   "source": [
    "test_question1_word_sequences = tokenizer.texts_to_sequences(test_question1)\n",
    "print(\"test_question1 is complete.\")\n",
    "test_question2_word_sequences = tokenizer.texts_to_sequences(test_question2)\n",
    "print(\"test_question2 is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in index: 1325\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(\"Words in index: %d\" % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Prepare the Train and Test\n",
    "\n",
    "Following variable store the training and test data.\n",
    "<ol>\n",
    "    <li> ** Train ** </li>\n",
    "    <ol>\n",
    "    <li> <b> train_q1 :</b> Contains first question</li>\n",
    "    <li> <b> train_q2 :</b> Contains Second question</li>\n",
    "    </ol>\n",
    "    <li> ** Test ** </li>\n",
    "    <ol>\n",
    "    <li> <b> train_q1 :</b> Contains first question</li>\n",
    "    <li> <b> train_q1 :</b> Contains second question</li>\n",
    "    </ol>    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_q1 is complete.\n",
      "train_q2 is complete.\n"
     ]
    }
   ],
   "source": [
    "# Pad the questions so that they all have the same length.\n",
    "\n",
    "max_question_len = 36\n",
    "\n",
    "train_q1 = pad_sequences(train_question1_word_sequences, \n",
    "                              maxlen = max_question_len)\n",
    "print(\"train_q1 is complete.\")\n",
    "\n",
    "train_q2 = pad_sequences(train_question2_word_sequences, \n",
    "                              maxlen = max_question_len)\n",
    "print(\"train_q2 is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_q1 is complete.\n",
      "test_q2 is complete.\n"
     ]
    }
   ],
   "source": [
    "test_q1 = pad_sequences(test_question1_word_sequences, \n",
    "                             maxlen = max_question_len,\n",
    "                             padding = 'post',\n",
    "                             truncating = 'post')\n",
    "print(\"test_q1 is complete.\")\n",
    "\n",
    "test_q2 = pad_sequences(test_question2_word_sequences, \n",
    "                             maxlen = max_question_len,\n",
    "                             padding = 'post',\n",
    "                             truncating = 'post')\n",
    "print(\"test_q2 is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.is_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "nb_words = len(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the parameter\n",
    "\n",
    "units = 128 # Number of nodes in the Dense layers\n",
    "dropout = 0.25 # Percentage of nodes to drop\n",
    "nb_filter = 32 # Number of filters to use in Convolution1D\n",
    "filter_length = 3 # Length of filter for Convolution1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/krishna/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    }
   ],
   "source": [
    "inp1 = Input(shape=(max_question_len,), name='input_1')\n",
    "X = Embedding(input_dim= nb_words+1, \n",
    "              output_dim= embedding_dim, \n",
    "              input_length=max_question_len, \n",
    "              trainable= False\n",
    "             )(inp1)\n",
    "X = Conv1D(filters= nb_filter, kernel_size= filter_length, padding='same')(X)\n",
    "X = BatchNormalization()(X)\n",
    "X = Activation('relu')(X)\n",
    "X = Dropout(dropout)(X)\n",
    "\n",
    "X = Conv1D(filters= nb_filter, kernel_size= filter_length, padding='same')(X)\n",
    "X = BatchNormalization()(X)\n",
    "X = Activation('relu')(X)\n",
    "X = Dropout(dropout)(X)\n",
    "out1 = Flatten()(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp2 = Input(shape=(max_question_len,), name='input_2')\n",
    "X = Embedding(input_dim= nb_words+1, \n",
    "              output_dim= embedding_dim, \n",
    "              input_length=max_question_len, \n",
    "              trainable= False\n",
    "             )(inp2)\n",
    "X = Conv1D(filters= nb_filter, kernel_size= filter_length, padding='same')(X)\n",
    "X = BatchNormalization()(X)\n",
    "X = Activation('relu')(X)\n",
    "X = Dropout(dropout)(X)\n",
    "\n",
    "X = Conv1D(filters= nb_filter, kernel_size= filter_length, padding='same')(X)\n",
    "X = BatchNormalization()(X)\n",
    "X = Activation('relu')(X)\n",
    "X = Dropout(dropout)(X)\n",
    "out2 = Flatten()(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp3 = Input(shape=(max_question_len,), name='input_3')\n",
    "X = Embedding(input_dim= nb_words+1, \n",
    "              output_dim= embedding_dim, \n",
    "              input_length=max_question_len, \n",
    "              trainable= False\n",
    "             )(inp3)\n",
    "X = TimeDistributed(Dense(embedding_dim))(X)\n",
    "X = BatchNormalization()(X)\n",
    "X = Activation('relu')(X)\n",
    "X = Dropout(dropout)(X)\n",
    "out3 = Lambda(lambda x: K.max(x, axis=1), output_shape=(embedding_dim,))(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp4 = Input(shape=(max_question_len,), name='input_4')\n",
    "X = Embedding(input_dim= nb_words+1, \n",
    "              output_dim= embedding_dim, \n",
    "              input_length=max_question_len, \n",
    "              trainable= False\n",
    "             )(inp4)\n",
    "X = TimeDistributed(Dense(embedding_dim))(X)\n",
    "X = BatchNormalization()(X)\n",
    "X = Activation('relu')(X)\n",
    "X = Dropout(dropout)(X)\n",
    "out4 = Lambda(lambda x: K.max(x, axis=1), output_shape=(embedding_dim,))(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge12 = concatenate([out1, out2], name='merge_layer_1')\n",
    "X = Dense(units=units*2)(merge12)\n",
    "X = BatchNormalization()(X)\n",
    "X = Activation('relu')(X)\n",
    "X = Dropout(dropout)(X)\n",
    "\n",
    "X = Dense(units=units)(X)\n",
    "X = BatchNormalization()(X)\n",
    "X = Activation('relu')(X)\n",
    "model12 = Dropout(dropout)(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge34 = concatenate([out3, out4], name='merge_layer_2')\n",
    "X = Dense(units=units*2)(merge34)\n",
    "X = BatchNormalization()(X)\n",
    "X = Activation('relu')(X)\n",
    "X = Dropout(dropout)(X)\n",
    "\n",
    "X = Dense(units=units)(X)\n",
    "X = BatchNormalization()(X)\n",
    "X = Activation('relu')(X)\n",
    "model34 = Dropout(dropout)(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = concatenate([model12, model34], name='final_merge')\n",
    "X = Dense(units=units*2)(merged)\n",
    "X = BatchNormalization()(X)\n",
    "X = Activation('relu')(X)\n",
    "X = Dropout(dropout)(X)\n",
    "\n",
    "X = Dense(units=units)(X)\n",
    "X = BatchNormalization()(X)\n",
    "X = Activation('relu')(X)\n",
    "X = Dropout(dropout)(X)\n",
    "\n",
    "X = Dense(units=units)(X)\n",
    "X = BatchNormalization()(X)\n",
    "X = Activation('relu')(X)\n",
    "X = Dropout(dropout)(X)\n",
    "\n",
    "X = Dense(units=1)(X)\n",
    "X = BatchNormalization()(X)\n",
    "out = Activation('sigmoid')(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[inp1, inp2, inp3, inp4], outputs=out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_best_weights = 'question_pairs_weights.h5'\n",
    "\n",
    "callbacks = [ModelCheckpoint(save_best_weights, monitor='val_loss', save_best_only=True),\n",
    "             EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')]\n",
    "\n",
    "history = model.fit([train_q1, train_q2, train_q1, train_q2],\n",
    "                    y_train,\n",
    "                    batch_size=256,\n",
    "                    epochs=2, \n",
    "                    validation_split=0.15,\n",
    "                    verbose=True,\n",
    "                    shuffle=True,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the summary statistics\n",
    "summary_stats = pd.DataFrame({'epoch': [ i + 1 for i in history.epoch ],\n",
    "                              'train_acc': history.history['acc'],\n",
    "                              'valid_acc': history.history['val_acc'],\n",
    "                              'train_loss': history.history['loss'],\n",
    "                              'valid_loss': history.history['val_loss']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(summary_stats.train_loss) # blue\n",
    "plt.plot(summary_stats.valid_loss) # green\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum validation loss during the training\n",
    "min_loss, idx = min((loss, idx) for (idx, loss) in enumerate(history.history['val_loss']))\n",
    "print('Minimum loss at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(min_loss))\n",
    "min_loss = round(min_loss, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the best weights\n",
    "#model.load_weights(save_best_weights)\n",
    "predictions = model.predict([test_q1, test_q2, test_q1, test_q2], verbose = True, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create submission\n",
    "submission = pd.DataFrame(predictions, columns=['is_duplicate'])\n",
    "submission.insert(0, 'test_id', test.test_id)\n",
    "file_name = 'submission_{}.csv'.format(min_loss)\n",
    "submission.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('quora_model.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
